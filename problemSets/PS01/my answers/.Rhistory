incumbents_subset <- read_csv("GitHub/StatsI_Fall2023/datasets/incumbents_subset.csv")
View(incumbents_subset)
head(incumbents_subset)
Regression_1 <- lm(voteshare ~ difflog, data = incumbents_subset)
summary(Regression_1)
Regression_2 <- lm(presvote ~ difflog, data = incumbents_subset)
summary(Regression_2)
Regression_3 <- lm(voteshare ~ presvote, data = incumbents_subset)
summary(Regression_3)
Regression_5 <- lm(voteshare ~ difflog + presvote, data = incumbents_subset)
summary(Regression_5)
#Run a regression model where the outcome variable is voteshare and
#explanatory variable difflog
Regression_1 <- lm(voteshare ~ difflog, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_1)
#Run a regression model where the outcome variable is presvote and
#the explanatory variable is difflog.
Regression_2 <- lm(presvote ~ difflog, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_2)
#Run a regression model where the outcome variable is voteshare and
#the explanatory variable is presvote.
Regression_3 <- lm(voteshare ~ presvote, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_3)
#Run a regression where the outcome variable is the incumbent's voteshare
#and the explanatory variables are difflog and presvote.
Regression_5 <- lm(voteshare ~ difflog + presvote, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_5)
head(incumbents_subset)
#Question 1
#1.- Run a regression model where the outcome variable is voteshare and
#explanatory variable difflog
Regression_1 <- lm(voteshare ~ difflog, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_1)
library(ggplot2)
#2. - Create a scatterplot with regression line
ggplot(incumbents_subset, aes(x = difflog, y = voteshare)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Scatterplot of Regression 1",
x = "Difference in Campaign Spending (difflog)",
y = "Incumbent Vote Share")
# Save Scatterplot as an image
ggsave("Scatterplot of Regression 1.pdf",
plot = Scatterplot of Regression 1.pdf,
pdf("Scatterplot of Regression 1.pdf")
# Save Scatterplot as an image
ggsave("Scatterplot_of_Regression_1.pdf",
plot = Scatterplot of Regression 1.pdf,
# Save Scatterplot as an image
ggsave("Scatterplot_of_Regression_1.pdf",
plot = Scatterplot_of_Regression_1.pdf,
width = 6, height = 4, units = "in")
#2. - Create a scatterplot with regression line
ScatterplotRegression1<-ggplot(incumbents_subset, aes(x = difflog, y = voteshare)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Scatterplot of Regression 1",
x = "Difference in Campaign Spending (difflog)",
y = "Incumbent Vote Share")
# Save Scatterplot as an image
ggsave("Scatterplot_of_Regression_1.pdf",
plot = ScatterplotRegression1.pdf,
width = 6, height = 4, units = "in")
# Save Scatterplot as an image
ggsave("Scatterplot_of_Regression_1.pdf",
plot = ScatterplotRegression1,
width = 6, height = 4, units = "in")
getwd()
#Question 4
#1. - Run a regression with residuals_1 as the outcome
# and residuals_2 as the explanatory variable
regression_4_resid <- lm(residuals_1 ~ residuals_2)
#Question 2
#1. - Run a regression model where the outcome variable is presvote and
#the explanatory variable is difflog.
Regression_2 <- lm(presvote ~ difflog, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_2)
#2. - Create a scatterplot with regression line
ggplot(incumbents_subset, aes(x = difflog, y = presvote)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "green") +
labs(title = "Scatterplot of Regression 2",
x = "Difference in Campaign Spending (difflog)",
y = "Presidential Vote Share")
#3. - Save the residuals of the model in a separate object.
residuals_2 <- resid(Regression_2)
#2. - Create a scatterplot with regression line
ScatterplotRegression1<-ggplot(incumbents_subset, aes(x = difflog, y = presvote)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "green") +
labs(title = "Scatterplot of Regression 2",
x = "Difference in Campaign Spending (difflog)",
y = "Presidential Vote Share")
ggsave("Scatterplot_of_Regression_2.pdf",
plot = ScatterplotRegression1,
width = 6, height = 4, units = "in")
#Get summary of model with coefficient estimates
summary(Regression_2)
#Question 3
#1. - Run a regression model where the outcome variable is voteshare and
#the explanatory variable is presvote.
Regression_3 <- lm(voteshare ~ presvote, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_3)
#2. - Create a scatterplot with regression line
ScatterplotRegression3<-ggplot(incumbents_subset, aes(x = presvote, y = voteshare)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(title = "Scatterplot of Regression 3",
x = "Presidential Vote Share",
y = "Incumbent Vote Share")
# Save Scatterplot as an image
ggsave("Scatterplot_of_Regression_3.pdf",
plot = ScatterplotRegression3,
width = 6, height = 4, units = "in")
#Get summary of model with coefficient estimates
summary(Regression_3)
#Question 4
#1. - Run a regression with residuals_1 as the outcome
# and residuals_2 as the explanatory variable
regression_4_resid <- lm(residuals_1 ~ residuals_2)
#3. - Save the residuals of the model in a separate object.
residuals_1 <- resid(Regression_1)
#3. - Save the residuals of the model in a separate object.
residuals_2 <- resid(Regression_2)
#Question 4
#1. - Run a regression with residuals_1 as the outcome
# and residuals_2 as the explanatory variable
regression_4_resid <- lm(residuals_1 ~ residuals_2)
# View the summary of the regression with residuals
summary(regression_4_resid)
#Question 4
#1. - Run a regression with residuals_1 as the outcome
# and residuals_2 as the explanatory variable
Regression_4_resid <- lm(residuals_1 ~ residuals_2)
# View the summary of the regression with residuals
summary(regression_4_resid)
ScatterplotRegression4<ggplot(incumbents_subset, aes(x = residuals_2, y = residuals_1)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "purple") +
labs(title = "Scatterplot of Regression 4",
x = "Residuals from Regression 2",
y = "Residuals from Regression 1")
ScatterplotRegression4<ggplot(incumbents_subset, aes(x = residuals_2, y = residuals_1)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "purple") +
labs(title = "Scatterplot of Regression 4",
x = "Residuals from Regression 2",
y = "Residuals from Regression 1")
Regression_4_resid <- lm(residuals_1 ~ residuals_2)
# View the summary of the regression with residuals
summary(Regression_4_resid)
ScatterplotRegression4<ggplot(incumbents_subset, aes(x = residuals_2, y = residuals_1)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "purple") +
labs(title = "Scatterplot of Regression 4",
x = "Residuals from Regression 2",
y = "Residuals from Regression 1")
ScatterplotRegression4<-<ggplot(incumbents_subset, aes(x = residuals_2, y = residuals_1)) +
ScatterplotRegression4<-ggplot(incumbents_subset, aes(x = residuals_2, y = residuals_1)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "purple") +
labs(title = "Scatterplot of Regression 4",
x = "Residuals from Regression 2",
y = "Residuals from Regression 1")
ggsave("Scatterplot_of_Regression_4.pdf",
plot = ScatterplotRegression4,
width = 6, height = 4, units = "in")
Regression_5 <- lm(voteshare ~ difflog + presvote, data = incumbents_subset)
#Get summary of model with coefficient estimates
summary(Regression_5)
# View the summary of the regression with residuals
summary(Regression_4_resid)
#Get summary of model with coefficient estimates
summary(Regression_5)
#Exploring the data
summary(incumbents_subset)
#Correlation Matrix
cor_matrix <- cor(incumbents_subset[, c("voteshare", "difflog", "presvote")])
summary(cor_matrix)
vif_values <- vif(Regression_5)
# Calculate VIF values
library(car)
# Calculate VIF values
install.packages("car")
library(car)
vif_values <- vif(Regression_5)
print(vif_values)
install.packages(car)
library(car)
data(Prestige)
view(Prestige)
head(Prestige)
summary(Prestige)
# Create a new variable 'professional' by recoding the 'type' variable
Prestige$professional <- ifelse(Prestige$type %in% c("prof", "bc"), 1, 0)
# View the updated dataset
head(Prestige)
# Run a linear model with prestige as the outcome
Regression_1 <- lm(prestige ~ income * professional, data = Prestige)
# Display the summary of the model
summary(Regression_1)
install.packages(car)
library(car)
data(Prestige)
help(Prestige)
head(Prestige)
summary(Prestige)
# Create a new variable 'professional' by recoding the 'type' variable
Prestige$professional <- ifelse(Prestige$type %in% c("prof", "bc"), 1, 0)
# View the updated dataset
head(Prestige)
# Run a linear model with prestige as the outcome
Regression_1 <- lm(prestige ~ income * professional, data = Prestige)
# Display the summary of the model
summary(Regression_1)
B1 <- 0.042
SEb1 <- 0.016
N <- 131
# Calculate the test statistic
test_statistic <- B1 / SEb1
# Degrees of freedom
df <- N - 1
# Two-tailed test, so multiply by 2
p_value <- 2 * pt(-abs(test_statistic), df)
# Compare p-value to significance level (e.g., 0.05)
if (p_value < 0.05) {
print("Reject the null hypothesis: Having yard signs in a precinct affects vote share.")
} else {
print("Fail to reject the null hypothesis: No evidence that yard signs affect vote share.")
}
# Option B:
B2 <- 0.042
SEb2 <- 0.013
# Calculate the test statistic
test_statistic <- B2 / SEb2
# Degrees of freedom
df <- N - 1
# Two-tailed test, so multiply by 2
p_value <- 2 * pt(-abs(test_statistic), df)
# Compare p-value to significance level (e.g., 0.05)
if (p_value < 0.05) {
print("Reject the null hypothesis: Being next to precincts with yard signs affects vote share.")
} else {
print("Fail to reject the null hypothesis: No evidence that being adjacent to yard signs affects vote share.")
}
head(Prestige)
BGDP <- -2
Se_GDP <- 0.00007
critical_value <- qnorm(0.975)  # For a 95% confidence interval
# Calculate margin of error
margin_of_error <- critical_value * se_GDP
# Calculate margin of error
margin_of_error <- critical_value * Se_GDP
confidence_interval <- c(beta_GDP - margin_of_error, beta_GDP + margin_of_error)
confidence_interval <- c(BGDP - margin_of_error, BGDP + margin_of_error)
# Print the result
cat("95% Confidence Interval for the effect of GDP on FDI:", confidence_interval, "\n")
mean_Education <- 12.04
mean_GDP <- 6378.56
range_GDP <- 41031.78 - 6378.56
# Calculate standard deviation of GDP
std_dev_GDP <- range_GDP / 2
low_GDP <- mean_GDP - std_dev_GDP
high_GDP <- mean_GDP + std_dev_GDP
predicted_FDI_low <- BGDP * low_GDP + mean_Education
predicted_FDI_high <- BGDP * high_GDP + mean_Education
# Difference in predicted FDI
difference_in_predicted_FDI <- predicted_FDI_high - predicted_FDI_low
# Print the result
cat("Difference in predicted FDI between low and high values of GDP:", difference_in_predicted_FDI, "\n")
#Option C
# Given values
mean_Education <- 12.04
# Calculate mean GDP as the midpoint of the range
mean_GDP <- (6378.56 + 41031.78) / 2
# Calculate standard deviation of GDP
std_dev_GDP <- (41031.78 - 6378.56) / 2
low_GDP <- mean_GDP - std_dev_GDP
high_GDP <- mean_GDP + std_dev_GDP
predicted_FDI_low <- BGDP * low_GDP + mean_Education
predicted_FDI_high <- BGDP * high_GDP + mean_Education
# Difference in predicted FDI
difference_in_predicted_FDI <- predicted_FDI_high - predicted_FDI_low
# Print the result
cat("Difference in predicted FDI between low and high values of GDP:", difference_in_predicted_FDI, "\n")
B0 <- 2.32
B_well_depth <- 0.07
B_dist100 <- -5.49
B_well_depth_dist100 <- 0.49
dist100_1 <- 0.36
dist100_2 <- 2.08
well_depth <- 1  # Assuming both households have a deep well
arsenic_level_1 <- B0 + (B_well_depth * well_depth) + (B_dist100 * dist100_1) + (B_well_depth_dist100 * well_depth * dist100_1)
arsenic_level_2 <- B0 + (B_well_depth * well_depth) + (B_dist100 * dist100_2) + (B_well_depth_dist100 * well_depth * dist100_2)
# Calculate average difference
average_difference <- (arsenic_level_1 - arsenic_level_2) / 2
# Print the result
cat("Average Difference in Arsenic Levels:", average_difference, "\n")
library(tidyverse) # load our packages here
library(rvest)
library(xml2)
html <- read_html(bowlers)
xml_structure(html)
# We use the read_html() function from rvest to read in the html
bowlers <- "https://stats.espncricinfo.com/ci/content/records/93276.html"
html <- read_html(bowlers)
html
xml_structure(html)
capture.output(xml_structure(html))
html %>%
html_nodes(table) # try searching for the table node
# html nodes using html_nodes()
html %>%
html_nodes("table") # try searching for the table node
html %>%
html_nodes(".ds-table") # try searching using the class (add a dot)
tab1 <- html %>%
html_nodes(xpath = "//table[position()=1}")
tab1 <- html %>%
html_nodes(xpath = "//table[position()=1]")
view(tab1)
tab2 <- tab1 %>%
html_nodes(xpath = "//thead | //tabble/tbody")
view(tab2)
tab2 <- tab1 %>%
html_nodes(xpath = "//table/thead | //tabble/tbody")
view(tab2)
view(tab2)
head(dat)
view(table1)
# We now have an object containing 2 lists. With a bit of work we can extract
# the text we want as a vector:
heads <- tab2[1] %>%
html_nodes(xpath = "") %>%
html_text()
## Packages
library(tidyverse) # load our packages here
library(rvest)
library(xml2)
# We use the read_html() function from rvest to read in the html
bowlers <- "https://stats.espncricinfo.com/ci/content/records/223646.html"
html <- read_html(bowlers)
html
xml_structure(html)
capture.output(xml_structure(html))
html %>%
html_nodes("table") # try searching for the table node
html %>%
html_nodes(".ds-table") # try searching using the class (add a dot)
dat %>%
filter(grepl("ENG|AUS", Player)) %>%
ggplot(aes(Balls, Wkts)) +
geom_text(aes(label = Player)) +
geom_smooth(method = "lm")
dat <-
dat %>%
filter(grepl("ENG|AUS", Player)) %>%
ggplot(aes(Balls, Wkts)) +
geom_text(aes(label = Player)) +
geom_smooth(method = "lm")
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
gu_api_key() # run this interactive function
dat <- gu_content(query = "", from_date = "") # making a tibble
dat <- gu_content(query = "ukraine", from_date = "2020-01-01") # making a tibble
dat <- gu_content(query = "ukraine", from_date = "2024-01-01") # making a tibble
saveRDS(dat, "data/df2023")
df <- dat
head(df) # checking our tibble
df <- df[] #
which(duplicated(df$web_title) == TRUE) # sometimes there are duplicates...
df <- df[!duplicated(df$web_title),] # which we can remove
corpus_ukr <- corpus(df,
docid_field = "web_title",
text_field = "body_text") #
# Checking our corpus
summary(corpus_ukr, 5)
as.character(corpus_ukr)[1]
test <- as.character(corpus_ukr)[1] # make a test object
stri_replace_first(test,
replacement = "", # nothing here (i.e. we're removing)
regex = "^.+?\"") #try to write the correct regex - this may help: https://www.rexegg.com/regex-quickstart.html
# Sometimes there's also boilerplate at the end of an article after a big centre dot.
as.character(corpus_ukr)[which(grepl("\u2022.+$", corpus_ukr))[1]]
# We could get rid of all that too with a different function
test <- as.character(corpus_ukr)[which(grepl("\u2022.+$", corpus_ukr))[1]]
stri_replace_last(test,
replacement = "",
regex = "\u2022.+$")
toks <- quanteda::tokens(corpus_ukr,
remove_punct = TRUE,
remove_symbols = TRUE)
toks <- tokens_tolower(toks) # lowercase tokens
print(toks[10]) # print lowercase tokens from the 10th article in corpus.
stop_list <- stopwords("english") # load English stopwords from quanteda
head(stop_list)
# The tokens_remove() function allows us to apply the stop_list to our toks object
toks <- tokens_remove(toks, stop_list)
toks[10] # print list of tokens from 10th article without stop words.
## 5.a. Normalising (or stemming) the tokens
# Now we'll stem the words using the tokens_wordstem() function
stem_toks <- tokens_wordstem(toks)
stem_toks[10] # print stemmed tokens from 10th document - notice any differences?
toks_list <- as.list(toks)
# ii. Apply the lemmatize_words function from textstem to the list of tokens
lemma_toks <- lapply(toks_list, lemmatize_words)
# iii. Convert the list of lemmatized tokens back to a quanteda tokens object
lemma_toks <- as.tokens(lemma_toks)
# i. Identify collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
# ii. Choose which to keep
keep_coll_list <- collocations$collocation[1:20]
keep_coll_list
# iii. Apply to tokens object
comp_tok <- tokens_compound(lemma_toks, keep_coll_list)
# Convert to dfm...
dfm_ukr <- dfm(comp_tok)
saveRDS(dfm_ukr, "data/dfm")
# We'll leave operations on the dfm until next time, but to give a preview, here are
# some functions we can use to analyse the dfm.
topfeatures(dfm_ukr)
dfm_ukr %>%
dfm_trim(min_termfreq = 3) %>%
textplot_wordcloud(min_size = 1, max_size = 10, max_words = 100)
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Problem 1
#####################
set.seed(123)
# create empirical distribution of observed data
ECDF <- ecdf(data)
# Define the Kolmogorov-Smirnov test function
ks_test <- function(data)
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# Define the Kolmogorov-Smirnov test function
ks_test <- function(data){
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# Calculate p-value using Kolmogorov-Smirnov CDF
n <- length(data)
p_value <- 1 - pnorm(sqrt(n) * D)
# Return the test statistic and p-value
return(list(statistic = D, p_value = p_value))}
# Generate 1,000 Cauchy random variables
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
# Perform the Kolmogorov-Smirnov test
result <- ks_test(data)
print(result)
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
ols_objective <- function(beta, x, y) {
y_pred <- beta[1] + beta[2] * x
residuals <- y - y_pred
sum(residuals^2)}
initial_guess <- c(0, 0)  # Initial guess for coefficients
fit_bfgs <- optim(par = initial_guess, fn = ols_objective, x = data$x, y = data$y, method = "BFGS")
# Extract coefficients from BFGS result
coefficients_bfgs <- fit_bfgs$par
# Compare with lm
lm_result <- lm(y ~ x, data = data)
coefficients_lm <- coef(lm_result)
# Print results
cat("Coefficients from BFGS (Newton-Raphson):\n")
print(coefficients_bfgs)
cat("\nCoefficients from lm (Ordinary Least Squares):\n")
print(coefficients_lm)
