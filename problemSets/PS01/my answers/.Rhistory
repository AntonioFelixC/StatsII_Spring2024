html %>%
html_nodes("table") # try searching for the table node
html %>%
html_nodes(".ds-table") # try searching using the class (add a dot)
tab1 <- html %>%
html_nodes(xpath = "//table[position()=1}")
tab1 <- html %>%
html_nodes(xpath = "//table[position()=1]")
view(tab1)
tab2 <- tab1 %>%
html_nodes(xpath = "//thead | //tabble/tbody")
view(tab2)
tab2 <- tab1 %>%
html_nodes(xpath = "//table/thead | //tabble/tbody")
view(tab2)
view(tab2)
head(dat)
view(table1)
# We now have an object containing 2 lists. With a bit of work we can extract
# the text we want as a vector:
heads <- tab2[1] %>%
html_nodes(xpath = "") %>%
html_text()
## Packages
library(tidyverse) # load our packages here
library(rvest)
library(xml2)
# We use the read_html() function from rvest to read in the html
bowlers <- "https://stats.espncricinfo.com/ci/content/records/223646.html"
html <- read_html(bowlers)
html
xml_structure(html)
capture.output(xml_structure(html))
html %>%
html_nodes("table") # try searching for the table node
html %>%
html_nodes(".ds-table") # try searching using the class (add a dot)
dat %>%
filter(grepl("ENG|AUS", Player)) %>%
ggplot(aes(Balls, Wkts)) +
geom_text(aes(label = Player)) +
geom_smooth(method = "lm")
dat <-
dat %>%
filter(grepl("ENG|AUS", Player)) %>%
ggplot(aes(Balls, Wkts)) +
geom_text(aes(label = Player)) +
geom_smooth(method = "lm")
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
gu_api_key() # run this interactive function
dat <- gu_content(query = "", from_date = "") # making a tibble
dat <- gu_content(query = "ukraine", from_date = "2020-01-01") # making a tibble
dat <- gu_content(query = "ukraine", from_date = "2024-01-01") # making a tibble
saveRDS(dat, "data/df2023")
df <- dat
head(df) # checking our tibble
df <- df[] #
which(duplicated(df$web_title) == TRUE) # sometimes there are duplicates...
df <- df[!duplicated(df$web_title),] # which we can remove
corpus_ukr <- corpus(df,
docid_field = "web_title",
text_field = "body_text") #
# Checking our corpus
summary(corpus_ukr, 5)
as.character(corpus_ukr)[1]
test <- as.character(corpus_ukr)[1] # make a test object
stri_replace_first(test,
replacement = "", # nothing here (i.e. we're removing)
regex = "^.+?\"") #try to write the correct regex - this may help: https://www.rexegg.com/regex-quickstart.html
# Sometimes there's also boilerplate at the end of an article after a big centre dot.
as.character(corpus_ukr)[which(grepl("\u2022.+$", corpus_ukr))[1]]
# We could get rid of all that too with a different function
test <- as.character(corpus_ukr)[which(grepl("\u2022.+$", corpus_ukr))[1]]
stri_replace_last(test,
replacement = "",
regex = "\u2022.+$")
toks <- quanteda::tokens(corpus_ukr,
remove_punct = TRUE,
remove_symbols = TRUE)
toks <- tokens_tolower(toks) # lowercase tokens
print(toks[10]) # print lowercase tokens from the 10th article in corpus.
stop_list <- stopwords("english") # load English stopwords from quanteda
head(stop_list)
# The tokens_remove() function allows us to apply the stop_list to our toks object
toks <- tokens_remove(toks, stop_list)
toks[10] # print list of tokens from 10th article without stop words.
## 5.a. Normalising (or stemming) the tokens
# Now we'll stem the words using the tokens_wordstem() function
stem_toks <- tokens_wordstem(toks)
stem_toks[10] # print stemmed tokens from 10th document - notice any differences?
toks_list <- as.list(toks)
# ii. Apply the lemmatize_words function from textstem to the list of tokens
lemma_toks <- lapply(toks_list, lemmatize_words)
# iii. Convert the list of lemmatized tokens back to a quanteda tokens object
lemma_toks <- as.tokens(lemma_toks)
# i. Identify collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
# ii. Choose which to keep
keep_coll_list <- collocations$collocation[1:20]
keep_coll_list
# iii. Apply to tokens object
comp_tok <- tokens_compound(lemma_toks, keep_coll_list)
# Convert to dfm...
dfm_ukr <- dfm(comp_tok)
saveRDS(dfm_ukr, "data/dfm")
# We'll leave operations on the dfm until next time, but to give a preview, here are
# some functions we can use to analyse the dfm.
topfeatures(dfm_ukr)
dfm_ukr %>%
dfm_trim(min_termfreq = 3) %>%
textplot_wordcloud(min_size = 1, max_size = 10, max_words = 100)
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c(),  pkgTest)
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt")
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt",
colClasses = c("hsgrad" = "factor",
"nonwhite" = "factor",
"mhs" = "factor",
"fhs" = "factor",
"intact" = "factor"))
summary(graduation)
# Drop problematic cases
graduation <- graduation[-which(graduation$nsibs < 0),]
#  Option 3:
#  Coerce from a character vector to a logical vector
graduation$hsgrad <- as.logical(as.numeric(as.factor(graduation$hsgrad))-1)
#  Option 4:
#  Use ifelse() with as.logical()...
as.logical(ifelse(graduation$hsgrad == "Yes", 1, 0))
mod <- glm(hsgrad ~ ., # period functions as omnibus selector (kitchen sink additive model)
data = graduation,
family = "binomial")
mod <- glm(hsgrad ~ .,
data = graduation,
family = binomial(link = "logit")) # same as above (logit is default arg)
summary(mod)
## Likelihood ratio test
#  Create a null model
nullMod <- glm(hsgrad ~ 1, # 1 = fit an intercept only (i.e. sort of a "mean")
data = graduation,
family = "binomial")
#  Run an anova test on the model compared to the null model
anova(nullMod, mod, test = "Chisq")
anova(nullMod, mod, test = "LRT") # LRT is equivalent
##  Extracting confidence intervals (of the coefficients)
?confint
exp(confint(mod)) # Remember: transform to odds ratio using exp()
confMod <- data.frame(cbind(lower = exp(confint(mod)[,1]),
coefs = exp(coef(mod)),
upper = exp(confint(mod)[,2])))
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
# An option for making a data.frame of confidence intervals and coefficients
confMod <- data.frame(cbind(lower = exp(confint(mod)[,1]),
coefs = exp(coef(mod)),
upper = exp(confint(mod)[,2])))
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
tydverse
lapply(c("tidyverse"),  pkgTest)
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
?model.matrix
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
?model.matrix
model.matrix( ~ unique(nsibs), data = graduation) # I see a problem with the data here...
# As a side note, we can use unique() with model.matrix() to create a matrix
# of different combinations of factor levels to use with predict(). Though it's
# probably not the best approach...
model.matrix( ~ as.factor(unique(nsibs)), data = graduation)
# A better function to help with this is expand.grid()
with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
# Consider for instance if we had a model just consisting of factors:
mod2 <- glm(hsgrad ~ nonwhite + mhs + fhs,
data = graduation,
family = "binomial")
predicted_data <- with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
predicted_data <- cbind(predicted_data, predict(mod2,
newdata = predicted_data,
type = "response",
se = TRUE))
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("us_tweets.csv",
stringsAsFactors=FALSE,
encoding = "utf-8")
# Now we can use the code in Jeff's lecture to fill out the confidence intervals
# and predicted probability (see lecture)
predicted_data <- within(predicted_data,
{PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
?cut
graduation$nsibs_cut <- cut(graduation$nsibs,
breaks = c(0, 0.9, 1, 3, Inf),
include.lowest = TRUE,
labels = c("None", "One", "Two_Three", "FourPlus"))
mod3 <- glm(hsgrad ~.,
data = graduation[,!names(graduation) %in% c("nsibs", "nsibs_f")],
family = "binomial")
summary(mod3)
summary(mod)
# Extract confidence intervals around the estimates
confMod3 <- data.frame(cbind(lower = exp(confint(mod3)[,1]),
coefs = exp(coef(mod3)),
upper = exp(confint(mod3)[,2])))
# Plot the estimates and confidence intervals
ggplot(data = confMod3, mapping = aes(x = row.names(confMod3), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
scale_y_continuous(breaks = seq(0,8,1)) +
labs(x = "Terms", y = "Coefficients")
# Insert code here
nrow(data)
ndata <- data[which(data$is_retweet == FALSE),]
nrow(ndata)
trump <- ndata[which(), ]# Fill in the gaps
#trump <- ndata[which(), ]# Fill in the gaps
trump <- ndata[which(ndata$screen_name == "realDonaldTrump"), ]
nrow(trump)
#sum(grepl("", trump$text, ignore.case = TRUE)) # Adapt this code as needed
sum(grepl("!", trump$text, ignore.case = TRUE))
sum(grepl("\\bwin?\\b|\\bwon\\b", trump$text, ignore.case = TRUE))
sum(grepl("\\bemploy?\\b|\\bunemploy?\\b", trump$text, ignore.case = TRUE))
sum(grepl("immigration|immigrant?|border", trump$text, ignore.case = TRUE))
sum(grepl("hoax", trump$text, ignore.case = TRUE))
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <-
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_????() %>% # which function transforms to lower case?
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_????() %>% # which function transforms to lower case?
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
#  tokens_????() %>% # which function transforms to lower case?
#  tokens_????(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
#  tokens_????('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
# tokens_????('amp', valuetype = 'fixed', padding = TRUE)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords('english'), padding = TRUE) %>%
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_????(???, # fill in the blanks here
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
#  tokens_????() %>% # which function transforms to lower case?
#  tokens_????(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
#  tokens_????('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
# tokens_????('amp', valuetype = 'fixed', padding = TRUE)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords('english'), padding = TRUE) %>%
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks,
method = "lambda",
size = 2,
min_count = 10,
smoothing = 0.5
)
toks <- tokens_compound(toks, pattern = col[col$z > 3,])
toks <- tokens_remove(tokens(toks), "") # this code removes whitespace
# create dfm from tokens object
docfm <- dfm(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
docfm <- dfm_select(docfm, pattern = stopwords("en"), selection = "remove")
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
#  tokens_????() %>% # which function transforms to lower case?
#  tokens_????(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
#  tokens_????('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
# tokens_????('amp', valuetype = 'fixed', padding = TRUE)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords('english'), padding = TRUE) %>%
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks,
method = "lambda",
size = 2,
min_count = 10,
smoothing = 0.5
)
toks <- tokens_compound(toks, pattern = col[col$z > 3,])
toks <- tokens_remove(tokens(toks), "") # this code removes whitespace
# create dfm from tokens object
docfm <- dfm(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
docfm <- dfm_select(docfm, pattern = stopwords("en"), selection = "remove")
library(ggplot2)
# Trump plot
dfm_trump <- dfm_subset(docfm, screen_name == "realDonaldTrump")
dfm_freq <- textstat_frequency(dfm_trump, n = 30)
dfm_freq$feature <- with(dfm_freq, reorder(feature, -frequency))
ggplot(dfm_freq, aes(x = feature, y = frequency)) +
ggtitle("Feature frequency of Trump tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Pelosi plot
dfm_pelosi <- dfm_subset(docfm, screen_name == "SpeakerPelosi")
dfm_freq <- textstat_frequency(dfm_pelosi, n = 30)
dfm_freq$feature <- with(dfm_freq, reorder(feature, -frequency))
ggplot(dfm_freq, aes(x = feature, y = frequency)) +
ggtitle("Feature frequency of Pelosi tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#Trump keyness
head(keyness_stat, 30)
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
#Trump keyness
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
#Trump keyness
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
# Pelosi keyness
# your code here
keyness_stat <- textstat_keyness(dfm_keyness, target = "SpeakerPelosi")
#Trump keyness
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem", # an alternative method for lemmatizing
"lubridate" # working with dates
), pkgTest)
ukr22 <- readRDS("data/df2022")
ukr22 <- readRDS("data/df2022")
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
set.seed(123)
# Define the Kolmogorov-Smirnov test function
ks_test <- function(data){
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# Calculate p-value using Kolmogorov-Smirnov CDF
n <- length(data)
p_value <- 1 - pnorm(sqrt(n) * D)
# Return the test statistic and p-value
return(list(statistic = D, p_value = p_value))}
# Generate 1,000 Cauchy random variables
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
# Perform the Kolmogorov-Smirnov test
result <- ks_test(data)
print(result)
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Define the objective function for OLS
ols_objective <- function(beta, x, y) {
y_pred <- beta[1] + beta[2] * x
residuals <- y - y_pred
sum(residuals^2)}
# Use BFGS algorithm to minimize the objective function
initial_guess <- c(0, 0)  # Initial guess for coefficients
fit_bfgs <- optim(par = initial_guess, fn = ols_objective, x = data$x, y = data$y, method = "BFGS")
# Extract coefficients from BFGS result
coefficients_bfgs <- fit_bfgs$par
# Compare with lm
lm_result <- lm(y ~ x, data = data)
coefficients_lm <- coef(lm_result)
# Print results
cat("Coefficients from BFGS (Newton-Raphson):\n")
print(coefficients_bfgs)
cat("\nCoefficients from lm (Ordinary Least Squares):\n")
print(coefficients_lm)
print(coefficients_bfgs)
print(coefficients_lm)
