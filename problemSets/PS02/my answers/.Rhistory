knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("us_tweets.csv",
stringsAsFactors=FALSE,
encoding = "utf-8")
# Now we can use the code in Jeff's lecture to fill out the confidence intervals
# and predicted probability (see lecture)
predicted_data <- within(predicted_data,
{PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
?cut
graduation$nsibs_cut <- cut(graduation$nsibs,
breaks = c(0, 0.9, 1, 3, Inf),
include.lowest = TRUE,
labels = c("None", "One", "Two_Three", "FourPlus"))
mod3 <- glm(hsgrad ~.,
data = graduation[,!names(graduation) %in% c("nsibs", "nsibs_f")],
family = "binomial")
summary(mod3)
summary(mod)
# Extract confidence intervals around the estimates
confMod3 <- data.frame(cbind(lower = exp(confint(mod3)[,1]),
coefs = exp(coef(mod3)),
upper = exp(confint(mod3)[,2])))
# Plot the estimates and confidence intervals
ggplot(data = confMod3, mapping = aes(x = row.names(confMod3), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
scale_y_continuous(breaks = seq(0,8,1)) +
labs(x = "Terms", y = "Coefficients")
# Insert code here
nrow(data)
ndata <- data[which(data$is_retweet == FALSE),]
nrow(ndata)
trump <- ndata[which(), ]# Fill in the gaps
#trump <- ndata[which(), ]# Fill in the gaps
trump <- ndata[which(ndata$screen_name == "realDonaldTrump"), ]
nrow(trump)
#sum(grepl("", trump$text, ignore.case = TRUE)) # Adapt this code as needed
sum(grepl("!", trump$text, ignore.case = TRUE))
sum(grepl("\\bwin?\\b|\\bwon\\b", trump$text, ignore.case = TRUE))
sum(grepl("\\bemploy?\\b|\\bunemploy?\\b", trump$text, ignore.case = TRUE))
sum(grepl("immigration|immigrant?|border", trump$text, ignore.case = TRUE))
sum(grepl("hoax", trump$text, ignore.case = TRUE))
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <-
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_????() %>% # which function transforms to lower case?
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_????() %>% # which function transforms to lower case?
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
#  tokens_????() %>% # which function transforms to lower case?
#  tokens_????(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
#  tokens_????('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
# tokens_????('amp', valuetype = 'fixed', padding = TRUE)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords('english'), padding = TRUE) %>%
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_????(???, # fill in the blanks here
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
#  tokens_????() %>% # which function transforms to lower case?
#  tokens_????(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
#  tokens_????('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
# tokens_????('amp', valuetype = 'fixed', padding = TRUE)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords('english'), padding = TRUE) %>%
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks,
method = "lambda",
size = 2,
min_count = 10,
smoothing = 0.5
)
toks <- tokens_compound(toks, pattern = col[col$z > 3,])
toks <- tokens_remove(tokens(toks), "") # this code removes whitespace
# create dfm from tokens object
docfm <- dfm(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
docfm <- dfm_select(docfm, pattern = stopwords("en"), selection = "remove")
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
# create corpus
corpus <- corpus(ndata)
# create tokens object
#  tokens_????() %>% # which function transforms to lower case?
#  tokens_????(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
#  tokens_????('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
# tokens_????('amp', valuetype = 'fixed', padding = TRUE)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords('english'), padding = TRUE) %>%
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks,
method = "lambda",
size = 2,
min_count = 10,
smoothing = 0.5
)
toks <- tokens_compound(toks, pattern = col[col$z > 3,])
toks <- tokens_remove(tokens(toks), "") # this code removes whitespace
# create dfm from tokens object
docfm <- dfm(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
docfm <- dfm_select(docfm, pattern = stopwords("en"), selection = "remove")
library(ggplot2)
# Trump plot
dfm_trump <- dfm_subset(docfm, screen_name == "realDonaldTrump")
dfm_freq <- textstat_frequency(dfm_trump, n = 30)
dfm_freq$feature <- with(dfm_freq, reorder(feature, -frequency))
ggplot(dfm_freq, aes(x = feature, y = frequency)) +
ggtitle("Feature frequency of Trump tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Pelosi plot
dfm_pelosi <- dfm_subset(docfm, screen_name == "SpeakerPelosi")
dfm_freq <- textstat_frequency(dfm_pelosi, n = 30)
dfm_freq$feature <- with(dfm_freq, reorder(feature, -frequency))
ggplot(dfm_freq, aes(x = feature, y = frequency)) +
ggtitle("Feature frequency of Pelosi tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#Trump keyness
head(keyness_stat, 30)
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
#Trump keyness
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
#Trump keyness
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
# Pelosi keyness
# your code here
keyness_stat <- textstat_keyness(dfm_keyness, target = "SpeakerPelosi")
#Trump keyness
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem", # an alternative method for lemmatizing
"lubridate" # working with dates
), pkgTest)
ukr22 <- readRDS("data/df2022")
ukr22 <- readRDS("data/df2022")
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c(),  pkgTest)
## More on logits: visualising and goodness of fit
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt",
stringsAsFactors = TRUE)
xtabs(~ hsgrad + nonwhite, data = graduation)
with(graduation, table(hsgrad, nonwhite))
summary(graduation)
# Find errors
graduation[which(graduation$nsibs < 0),]
graduation <- graduation[-which(graduation$nsibs < 0),]
# Model one:
graduation$nsibs_cut <- cut(graduation$nsibs,
breaks = c(0, 0.9, 1, 3, Inf),
include.lowest = TRUE,
labels = c("None", "One", "Two_Three", "FourPlus"))
mod_1 <- glm(hsgrad ~.,
data = graduation[,!names(graduation) %in% c("nsibs")],
family = "binomial")
# A more parsimonious model
mod_2 <- glm(hsgrad ~ nsibs_cut + income + nonwhite,
data = graduation,
family = "binomial")
summary (mod_2)
# Make a data frame
predicted_data <- data.frame(
hsgrad = graduation$hsgrad,
mod_1_hat = mod_1$fitted.values,
mod_2_hat = mod_2$fitted.values
)
# Reorder and plot
predicted_data %>%
arrange(mod_1_hat) %>%
mutate(rank = row_number()) %>%
ggplot(aes(rank, mod_1_hat)) +
geom_point(aes(colour = hsgrad), alpha = 0.5) +
scale_y_continuous(limits = c(0,1))
# Reorder and plot
predicted_data %>%
arrange(mod_1_hat) %>%
mutate(rank = row_number()) %>%
ggplot(aes(rank, mod_1_hat)) +
geom_point(aes(colour = hsgrad), alpha = 0.5) +
scale_y_continuous(limits = c(0,1))
lapply(c("tidyverse", "car"),  pkgTest)
predicted_data %>%
arrange(mod_1_hat) %>%
mutate(rank = row_number()) %>%
ggplot(aes(rank, mod_1_hat)) +
geom_point(aes(colour = hsgrad), alpha = 0.5) +
scale_y_continuous(limits = c(0,1))
predicted_data %>%
arrange(mod_2_hat) %>%
mutate(rank = row_number()) %>%
ggplot(aes(rank, mod_2_hat)) +
geom_point(aes(colour = hsgrad), alpha = 0.5) +
scale_y_continuous(limits = c(0,1))
# McFadden's R squared
# Approach 1:
mod_1$null.deviance == mod_2$null.deviance
ll.null <- mod_1$null.deviance/-2
ll.fit_1 <- mod_1$deviance/-2
ll.fit_2 <- mod_2$deviance/-2
r.sq.mod_1 <- (ll.null-ll.fit_1)/ll.null
r.sq.mod_2 <- (ll.null-ll.fit_2)/ll.null
# Approach 2:
mod_null <- glm(hsgrad ~ 1, data = graduation, family = "binomial")
1 - logLik(mod_1)/logLik(mod_null)
1 - logLik(mod_2)/logLik(mod_null)
# P value:
1 - pchisq(2*(ll.fit_1 - ll.null), df = (length(mod_1$coefficients)-1))
1 - pchisq(2*(ll.fit_2 - ll.null), df = (length(mod_2$coefficients)-1))
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi",
"quanteda",
"lubridate",
"quanteda.textmodels",
"quanteda.textstats",
"caret", # For train/test split
"MLmetrics", # For ML
"doParallel"), # For parallel processing
pkgTest)
## 1. Acquire, read in and wrangle data
#gu_api_key() # run this interactive function
#dat <- gu_content(query = "Ukraine", from_date = "2022-01-01", to_date = "2022-07-01")
dat <- readRDS("data/df2023") # try this with different data.frames
dat <- readRDS("data/df2023") # try this with different data.frames
dat <- readRDS("data/df2023") # try this with different data.frames
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
# Structure of the dataset
str(climateSupport)
# Summary statistics of the dataset
summary(climateSupport)
# Dimensions of the dataset
dim(climateSupport)
# View first few rows of the dataset
head(climateSupport)
# Fit logistic regression model with interaction term
model_interaction <- glm(choice ~ countries * sanctions, data = climateSupport, family = binomial)
# Summary of the model
summary(model_interaction)
#The main effects of countries and sanctions are significant (p < 0.001),
#indicating that both variables have a significant effect on the likelihood
#of supporting the policy.
# Extract coefficient for the interaction term between countries and sanctions
interaction_coef <- coef(model_interaction)["countries.L:sanctions.L"]
# Interpretation of the coefficient
cat("The coefficient for the interaction term between countries and sanctions is:", interaction_coef, "\n\n")
cat("Interpretation:\n")
cat("For each one-unit increase in sanctions (from 5% to 15%), the odds of supporting the policy\n")
cat("are multiplied by exp(", interaction_coef, ") = ", exp(interaction_coef), "\n\n")
#For each one-unit increase in sanctions (from 5% to 15%),
#the odds of supporting the policy are multiplied by
#exp(-0.001754016) â‰ˆ 0.9982475.
#This implies that increasing sanctions from 5% to 15% is associated
#with a slight decrease in the odds of supporting the policy.
#However, the effect is minimal, indicating that the change in odds
#is not substantial.
# Define the values of countries and sanctions
countries_value <- "80 of 192"
sanctions_value <- "None"
# Predict the probability using the logistic regression model
predicted_probability <- predict(model_interaction,
newdata = data.frame(countries = countries_value, sanctions = sanctions_value),
type = "response")
# Print the estimated probability
cat("The estimated probability that an individual will support the policy if there are",
countries_value, "countries participating with no sanctions is:", predicted_probability, "\n")
#The estimated probability that an individual will support the policy
#if there are 80 of 192 countries participating with
#no sanctions is: 0.5252101
# Fit logistic regression model without the interaction term
model_no_interaction <- glm(choice ~ countries + sanctions, data = climateSupport, family = binomial)
# Perform likelihood ratio test
lrt <- anova(model_no_interaction, model_interaction, test = "Chisq")
# Print the results
print(lrt)
#In this case, the p-value (Pr(>Chi)) is 0.3912,
#which is greater than the significance level of 0.05.
#Therefore, we fail to reject the null hypothesis that the model
#without the interaction term is sufficient.
#This suggests that including the interaction term does not
#significantly improve the model's fit.
# Global null hypothesis and p-value
null_hypothesis <- "There is no relationship between the predictors and the likelihood of an individual supporting the policy."
p_value <- summary(model)$p.table[1, "Pr(>Chisq)"]
# Global null hypothesis and p-value
null_hypothesis <- "There is no relationship between the predictors and the likelihood of an individual supporting the policy."
p_value <- summary(model_interaction)$p.table[1, "Pr(>Chisq)"]
# Global null hypothesis and p-value
null_hypothesis <- "There is no relationship between the predictors and the likelihood of an individual supporting the policy."
p_value <- summary(model_interaction)$p.table[1, "Pr(>Chisq)"]
# Print the global null hypothesis and p-value
cat("Global Null Hypothesis:", null_hypothesis, "\n")
cat("p-value:", p_value, "\n")
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
# Structure of the dataset
str(climateSupport)
# Summary statistics of the dataset
summary(climateSupport)
# Dimensions of the dataset
dim(climateSupport)
# View first few rows of the dataset
head(climateSupport)
# Fit logistic regression model with interaction term
model_interaction <- glm(choice ~ countries * sanctions, data = climateSupport, family = binomial)
# Summary of the model
summary(model_interaction)
# Global null hypothesis and p-value
null_hypothesis <- "There is no relationship between the predictors and the likelihood of an individual supporting the policy."
p_value <- summary(model_interaction)$p.table[1, "Pr(>Chisq)"]
# Print the global null hypothesis and p-value
cat("Global Null Hypothesis:", null_hypothesis, "\n")
cat("p-value:", p_value, "\n")
# Global null hypothesis and p-value
null_hypothesis <- "There is no relationship between the predictors and the likelihood of an individual supporting the policy."
p_value <- summary(model_interaction)$p.table[9, "Pr(>|z|)"]  # Extracting the p-value from the summary
# Print the global null hypothesis and p-value
cat("Global Null Hypothesis:", null_hypothesis, "\n")
cat("p-value:", p_value, "\n")
# Print the global null hypothesis and p-value
cat("Global Null Hypothesis:", null_hypothesis, "\n")
cat("p-value:", p_value, "\n")
# Global null hypothesis and p-value
null_hypothesis <- "There is no relationship between the predictors and the likelihood of an individual supporting the policy."
p_value <- summary(model_interaction)$coefficients[1, "Pr(>|z|)"]
# Print the global null hypothesis and p-value
cat("Global Null Hypothesis:", null_hypothesis, "\n")
cat("p-value:", p_value, "\n")
# Extract coefficient for the interaction term between countries and sanctions
interaction_coef <- coef(model_interaction)["countries.L:sanctions.L"]
# Interpretation of the coefficient
cat("The coefficient for the interaction term between countries and sanctions is:", interaction_coef, "\n\n")
cat("Interpretation:\n")
cat("For each one-unit increase in sanctions (from 5% to 15%), the odds of supporting the policy\n")
cat("are multiplied by exp(", interaction_coef, ") = ", exp(interaction_coef), "\n\n")
#Question 2
# Extract coefficient for the interaction term between countries and sanctions
interaction_coef <- coef(model_interaction)["countries.L:sanctions.L"]
# Define the values of countries and sanctions
countries_value <- "80 of 192"
sanctions_value <- "None"
# Predict the probability using the logistic regression model
predicted_probability <- predict(model_interaction,
newdata = data.frame(countries = countries_value, sanctions = sanctions_value),
type = "response")
# Print the estimated probability
cat("The estimated probability that an individual will support the policy if there are",
countries_value, "countries participating with no sanctions is:", predicted_probability, "\n")
#2C
# Fit logistic regression model without the interaction term
model_no_interaction <- glm(choice ~ countries + sanctions, data = climateSupport, family = binomial)
# Perform likelihood ratio test
lrt <- anova(model_no_interaction, model_interaction, test = "Chisq")
# Print the results
print(lrt)
#2B
# Define the values of countries and sanctions
countries_value <- "80 of 192"
sanctions_value <- "None"
# Predict the probability using the logistic regression model
predicted_probability <- predict(model_interaction,
newdata = data.frame(countries = countries_value, sanctions = sanctions_value),
type = "response")
# Print the estimated probability
cat("The estimated probability that an individual will support the policy if there are",
countries_value, "countries participating with no sanctions is:", predicted_probability, "\n")
